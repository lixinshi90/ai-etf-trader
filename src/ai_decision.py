# -*- coding: utf-8 -*-
"""
AI å†³ç­–æ¨¡å—ï¼šè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ETFçš„ä¹°/å–/æŒæœ‰å†³ç­–ï¼ˆJSONæ ¼å¼ï¼‰
- ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_KEYï¼ˆæˆ– .env é…ç½®ï¼‰
- å¯é€šè¿‡ç¯å¢ƒå˜é‡ MODEL_NAME æŒ‡å®šæ¨¡å‹ï¼Œé»˜è®¤ gpt-4o-mini
- å¯é€šè¿‡ç¯å¢ƒå˜é‡ BASE_URL æ”¯æŒ OpenAI å…¼å®¹æœåŠ¡ï¼ˆDeepSeek/Groq/Mistral/BigModelç­‰ï¼‰
- å¯é€šè¿‡ TIMEOUT_SECONDS / MAX_RETRIES é…ç½®è¶…æ—¶ä¸é‡è¯•
- å¯é€šè¿‡ FALLBACK_MODELS é…ç½®å›é€€æ¨¡å‹ï¼ˆé€—å·åˆ†éš”ï¼‰
- æ”¯æŒå½“æ—¥å†³ç­–ç¼“å­˜ï¼šå¦‚æœå·²å­˜åœ¨åŒæ—¥å†³ç­–æ–‡ä»¶åˆ™ç›´æ¥å¤ç”¨ï¼Œå‡å°‘åå¤è®¡è´¹ä¸å¤±è´¥
- å°†å½“æ—¥çš„ System Prompt ä¸ User Message è½ç›˜ä¿å­˜ï¼Œä¾¿äºå®¡è®¡
- å°†å†³ç­–JSONè½ç›˜è‡³ decisions ç›®å½•
"""
from __future__ import annotations

import glob
import json
import os
import re
from datetime import datetime
from typing import Any, Dict, Optional, Tuple

from dotenv import load_dotenv

from .prompts import SYSTEM_PROMPT, build_user_message, FEW_SHOTS

# å»¶è¿Ÿå¯¼å…¥ä»¥æ›´å¥½åœ°ç»™å‡ºé”™è¯¯æç¤º
try:
    from openai import OpenAI
except Exception:  # pragma: no cover
    OpenAI = None


def _project_root() -> str:
    return os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))


def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


def _today_str() -> str:
    return datetime.now().strftime("%Y%m%d")


def _now_ts() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def _safe_json_parse(text: str) -> Dict[str, Any]:
    """å°½åŠ›ä»æ–‡æœ¬ä¸­è§£æå‡º JSON å¯¹è±¡ï¼ˆdictï¼‰ã€‚

    ç›®æ ‡ï¼šæ˜¾è‘—é™ä½ `Expecting ',' delimiter` ç­‰è§£æå¤±è´¥ç‡ã€‚

    æ”¯æŒï¼š
    - ```json ...``` ä»£ç å—
    - è¾“å‡ºå‰åå¤¹æ‚è§£é‡Šæ–‡å­—ï¼ˆä»é¦–ä¸ª { åˆ°æœ€åä¸€ä¸ª } æˆªå–ï¼‰
    - å¸¸è§è„å­—ç¬¦ï¼šBOMã€ä¸­æ–‡å¼•å·

    æ³¨æ„ï¼šè¿™é‡Œä¸åšâ€œè¯­ä¹‰ä¿®å¤â€ï¼Œåªåšè½»é‡è§„åˆ™æ¸…æ´—ï¼›ä»å¤±è´¥åˆ™æŠ›é”™ã€‚
    """
    if not text or not str(text).strip():
        raise ValueError("ç©ºå“åº”ï¼Œæ— æ³•è§£æJSON")

    raw = str(text).strip().lstrip("\ufeff")

    # 1) ä¼˜å…ˆæå– markdown json ä»£ç å—
    m = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", raw, flags=re.IGNORECASE)
    if m:
        candidate = m.group(1).strip()
    else:
        candidate = raw

    # 2) è‹¥ä¸æ˜¯ä»¥ { å¼€å¤´ï¼Œå°è¯•æˆªå–ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª }
    s = candidate.strip()
    if not s.startswith("{"):
        l = s.find("{")
        r = s.rfind("}")
        if l != -1 and r != -1 and r > l:
            s = s[l : r + 1]

    # 3) æ¸…æ´—å¸¸è§é—®é¢˜ï¼šä¸­æ–‡å¼•å·
    s = s.replace("â€œ", '"').replace("â€", '"').replace("â€˜", "'").replace("â€™", "'")

    # 4) è§£æ
    obj = json.loads(s)
    if not isinstance(obj, dict):
        raise ValueError("è§£æç»“æœä¸æ˜¯å¯¹è±¡")
    return obj


def _find_cached_decision(decisions_dir: str, etf_code: str, date_str: Optional[str] = None) -> Optional[Tuple[str, Dict[str, Any]]]:
    """æŸ¥æ‰¾å½“æ—¥è¯¥æ ‡çš„å·²æœ‰å†³ç­–æ–‡ä»¶ï¼Œè¿”å› (æ–‡ä»¶è·¯å¾„, å†³ç­–å¯¹è±¡) æˆ– Noneã€‚"""
    if date_str is None:
        date_str = _today_str()
    pattern = os.path.join(decisions_dir, f"{date_str}_{etf_code}_*.json")
    files = sorted(glob.glob(pattern), reverse=True)
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8") as f:
                obj = json.load(f)
            if isinstance(obj, dict) and obj.get("decision"):
                return fp, obj
        except Exception:
            continue
    return None


def get_ai_decision(
    etf_code: str,
    df,
    prompts_dir: Optional[str] = None,
    decisions_dir: Optional[str] = None,
    system_prompt: str = SYSTEM_PROMPT,
    model_name: Optional[str] = None,
    use_cache: bool = True,
    force_date: Optional[datetime.date] = None,
) -> Dict[str, Any]:
    """è·å–AIå†³ç­–ï¼ˆè¿”å›å­—å…¸ï¼ŒåŒ…å« decision/ confidence/ reasoning/ target_price/ stop_loss/ take_profitï¼‰
    - ä¼šå°† Prompt å’Œ å†³ç­–JSON è½ç›˜
    - é»˜è®¤ä¼˜å…ˆä½¿ç”¨å½“æ—¥ç¼“å­˜
    """
    # ç¯å¢ƒä¸ç›®å½•
    load_dotenv(override=True)  # ä¼˜å…ˆä»¥ .env è¦†ç›–å¤–éƒ¨ç¯å¢ƒï¼Œé¿å…æ··é…
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨ .env ä¸­é…ç½®æˆ–å¯¼å‡ºç¯å¢ƒå˜é‡")

    if model_name is None:
        model_name = os.getenv("MODEL_NAME", "gpt-4o-mini")

    base_url = os.getenv("BASE_URL")  # ä¾‹å¦‚ DeepSeek/Groq/BigModel å…¼å®¹ç«¯ç‚¹
    try:
        timeout_s = float(os.getenv("TIMEOUT_SECONDS", "120"))  # é»˜è®¤120ç§’
    except Exception:
        timeout_s = 120.0
    try:
        max_retries = int(os.getenv("MAX_RETRIES", "3"))  # é»˜è®¤3æ¬¡ï¼Œæ»¡è¶³â€œæœ€å¤šé‡è¯•3æ¬¡â€
    except Exception:
        max_retries = 3

    fallback_models_env = os.getenv("FALLBACK_MODELS", "gpt-4o")
    fallback_models = [m.strip() for m in fallback_models_env.split(",") if m.strip()]

    if prompts_dir is None:
        prompts_dir = os.path.join(_project_root(), "prompts")
    if decisions_dir is None:
        decisions_dir = os.path.join(_project_root(), "decisions")
    _ensure_dir(prompts_dir)
    _ensure_dir(decisions_dir)

    date_str = force_date.strftime("%Y%m%d") if force_date else _today_str()

    # å½“æ—¥ç¼“å­˜å‘½ä¸­åˆ™ç›´æ¥è¿”å›ï¼ˆä¼˜å…ˆå¤ç”¨ï¼Œé¿å…é‡å¤è®¡è´¹/å¤±è´¥ï¼‰
    if use_cache:
        cached = _find_cached_decision(decisions_dir, etf_code, date_str=date_str)
        if cached:
            _, cached_obj = cached
            return cached_obj

        # è¿›ä¸€æ­¥ï¼šå¦‚æœå½“å¤©å·²æœ‰ raw è¾“å‡ºä½†å†³ç­– json æ²¡è½ç›˜ï¼Œå¯åœ¨æ­¤æ‰©å±•â€œä» raw é‡å»ºâ€ã€‚
        # ç›®å‰å…ˆä¸è‡ªåŠ¨é‡å»ºï¼Œé¿å…è¯¯è§£æï¼›ä½† raw å·²ä¼šå†™å…¥ logs/llm_raw_* ä¾¿äºæ‰‹å·¥æ’æŸ¥ã€‚

    # æ„å»ºæ¶ˆæ¯
    user_message = build_user_message(etf_code, df)
    use_few = os.getenv("FEW_SHOT_ENABLED", "false").lower() == "true"
    messages = [{"role": "system", "content": system_prompt}]
    if use_few:
        messages += FEW_SHOTS
    messages.append({"role": "user", "content": user_message})

    # å®¢æˆ·ç«¯
    if OpenAI is None:  # pragma: no cover
        raise RuntimeError("openai åº“ä¸å¯ç”¨ï¼Œè¯· pip install openai")

    client_kwargs = {"api_key": api_key, "timeout": timeout_s, "max_retries": max_retries}
    if base_url:
        client_kwargs["base_url"] = base_url
    client = OpenAI(**client_kwargs)

    # BigModelï¼ˆopen.bigmodel.cnï¼‰ç­‰æœ‰äº›ç«¯ç‚¹å¯èƒ½ä¸æ”¯æŒ JSON Modeï¼Œè¿™é‡Œåšå…¼å®¹ï¼š
    use_json_mode = True
    if base_url and ("open.bigmodel.cn" in base_url.lower()):
        use_json_mode = False

    # ç»„ç»‡å€™é€‰æ¨¡å‹ï¼šä¸»æ¨¡å‹ + å›é€€æ¨¡å‹
    tried_models = []
    model_candidates = [model_name] + [m for m in fallback_models if m not in tried_models]

    import time as _time

    last_err: Optional[Exception] = None
    decision: Optional[Dict[str, Any]] = None

    for m in model_candidates:
        tried_models.append(m)
        for attempt in range(1, max_retries + 1):
            try:
                print(f"è°ƒç”¨æ¨¡å‹ {m}ï¼ˆç¬¬{attempt}æ¬¡å°è¯•ï¼‰...")
                kwargs = dict(
                    model=m,
                    messages=messages,
                    temperature=0.3,
                    timeout=timeout_s,
                )
                if use_json_mode:
                    kwargs["response_format"] = {"type": "json_object"}
                resp = client.chat.completions.create(**kwargs)
                raw_text = resp.choices[0].message.content if resp and resp.choices else ""

                # è®°å½•åŸå§‹è¾“å‡ºï¼ˆæˆåŠŸ/å¤±è´¥éƒ½å¯èƒ½éœ€è¦æ’æŸ¥ï¼‰ï¼Œé¿å…â€œçœ‹ä¸åˆ°æ¨¡å‹åˆ°åº•å›äº†ä»€ä¹ˆâ€
                logs_dir = os.path.join(_project_root(), "logs")
                _ensure_dir(logs_dir)
                raw_fp = os.path.join(logs_dir, f"llm_raw_{date_str}_{etf_code}_{_now_ts()}_{m}.txt")
                try:
                    with open(raw_fp, "w", encoding="utf-8") as f:
                        f.write(raw_text or "")
                except Exception:
                    pass

                decision = _safe_json_parse(raw_text)
                print(f"æ¨¡å‹ {m} è°ƒç”¨æˆåŠŸï¼")
                break
            except Exception as e:
                last_err = e
                if attempt < max_retries:
                    wait = 5 * (2 ** (attempt - 1))
                    print(f"âŒ æ¨¡å‹{m}è°ƒç”¨å¤±è´¥ï¼ˆç¬¬{attempt}æ¬¡ï¼‰ï¼š{e}")
                    print(f"â³ {wait}ç§’åé‡è¯•...")
                    _time.sleep(wait)
                else:
                    print(f"âŒ æ¨¡å‹{m}è°ƒç”¨å¤±è´¥ä¸”è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼š{e}")
                    if m != model_candidates[-1]:
                        print(f"ğŸ“Œ å°è¯•å›é€€æ¨¡å‹...")
        if decision is not None:
            model_name = m
            break

    if decision is None:
        # å…¨éƒ¨å°è¯•å¤±è´¥
        raise RuntimeError(f"æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ˆå·²å°è¯•ï¼š{model_candidates}ï¼‰ï¼š{last_err}")

    # åŸºç¡€å­—æ®µå…œåº•
    decision.setdefault("decision", "hold")
    decision.setdefault("confidence", 0.5)
    decision.setdefault("reasoning", "æ¨¡å‹æœªæä¾›è¯¦ç»†ç†ç”±")
    for k in ("target_price", "stop_loss", "take_profit"):
        decision.setdefault(k, None)

    # è½ç›˜
    date_str = _today_str()
    ts = _now_ts()
    prompt_file = os.path.join(prompts_dir, f"{date_str}_{etf_code}_{ts}_prompt.txt")
    decision_file = os.path.join(decisions_dir, f"{date_str}_{etf_code}_{ts}.json")

    with open(prompt_file, "w", encoding="utf-8") as f:
        f.write("System Prompt:\n" + system_prompt + "\n\n")
        f.write("User Message:\n" + user_message + "\n")

    with open(decision_file, "w", encoding="utf-8") as f:
        json.dump(decision, f, ensure_ascii=False, indent=2)

    return decision
